# Example configuration for local Ollama model
# Copy these settings to your .env file or export them before running

# LLM Configuration for Local Model
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:8b
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
LLM_STREAMING=false  # IMPORTANT: Disable streaming for local models to avoid hanging
OLLAMA_BASE_URL=http://localhost:11434

# Embedding Configuration (optional - can use local embeddings)
EMBED_PROVIDER=huggingface
EMBED_MODEL=BAAI/bge-small-en-v1.5

# Or keep OpenAI embeddings (faster and more reliable)
# EMBED_PROVIDER=openai
# EMBED_MODEL=text-embedding-3-small

# Required API Keys (keep your existing values)
LLAMA_CLOUD_API_KEY=your_llama_cloud_api_key_here
LLAMA_CLOUD_ORG_ID=your_org_id_here

# Optional: OpenAI API key (if using OpenAI embeddings or fallback)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Phoenix tracing
PHOENIX_API_KEY=YOUR_PHOENIX_API_KEY

# Instructions:
# 1. Make sure Ollama is running: ollama serve
# 2. Make sure the model is installed: ollama pull llama3.1:8b
# 3. Test connectivity: curl http://localhost:11434/api/tags
# 4. Run with: ./run_local_model.sh [pdf_file]
